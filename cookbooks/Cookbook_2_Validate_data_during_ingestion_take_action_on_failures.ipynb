{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d69dd0-cec7-4554-b4de-3f3db4d62add",
   "metadata": {},
   "source": [
    "# Cookbook 2: Validate data during ingestion (take action on failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b7fd6-f986-4d5e-a627-17c550e1348f",
   "metadata": {},
   "source": [
    "This cookbook showcases a sample GX data validation workflow characteristic of data ingestion at the start of the data pipeline. Data is loaded into a Pandas dataframe, cleaned, validated, and then ingested into a Postgres database table. This cookbook explores the validation workflow first in a notebook setting, then embedded within an Airflow pipeline.\n",
    "\n",
    "This cookbook features a scenario in which a subset of data fails validation and must be handled in the pipeline.\n",
    "\n",
    "This cookbook builds on [Cookbook 1: Validate data during ingestion (happy path)](Cookbook_1_Validate_data_during_ingestion_happy_path.ipynb) and focuses on how data validation failures can be programmatically handled in the pipeline based on GX Validation Results. This cookbook assumes basic familiarity with GX Core workflows; for a step-by-step explanation of the GX data validation workflow, refer to [Cookbook 1](Cookbook_1_Validate_data_during_ingestion_happy_path.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7e703b-73f9-4550-8759-5aabfb5f0669",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d187e1a6-915e-4a68-8630-e04a4f4bf98c",
   "metadata": {},
   "source": [
    "This tutorial features the `great_expectations` library.\n",
    "\n",
    "The `tutorial_code` module contains helper functions used within this notebook and the associated Airflow pipeline.\n",
    "\n",
    "The `airflow_dags` submodule is included so that you can inspect the code used in the related Airflow DAG directly from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20879e58-685f-4ed9-b1d3-56b3521c1a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import inspect\n",
    "\n",
    "import great_expectations as gx\n",
    "import great_expectations.expectations as gxe\n",
    "import pandas as pd\n",
    "\n",
    "import tutorial_code as tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4325870-7d39-48a1-a06e-da86903c20c9",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733a5bd9-0b7f-4707-83ba-9791a0f28a82",
   "metadata": {},
   "source": [
    "In this tutorial, you will clean and validate a dataset containing synthesized product data. The data is loaded from a CSV file into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8320005b-5560-40be-a797-0c3720036dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"/cookbooks/data/raw\")\n",
    "\n",
    "df_products_raw = pd.read_csv(DATA_DIR / \"products.csv\", encoding=\"unicode_escape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97df7e7-5b97-41f4-adf1-55d7d4fc678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {df_products_raw.shape[0]} product rows into dataframe.\\n\")\n",
    "\n",
    "display(df_products_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac1e31-7473-47da-9607-d96e968da270",
   "metadata": {},
   "source": [
    "## Examine destination tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e3fe6a-63e3-4f16-a328-96830e4c6de1",
   "metadata": {},
   "source": [
    "The product data will be normalized and loaded into multiple Postgres tables:\n",
    "* `products`\n",
    "* `product_category`\n",
    "* `product_subcategory`\n",
    "\n",
    "Examine the schema of the destination tables and compare to the initial schema and contents of the raw product data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b71f661-71bd-4ad6-bce0-0bdd6eb3e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.db.get_table_schema(table_name=\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a885e-759b-4af6-8bcd-bb5346983a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.db.get_table_schema(table_name=\"product_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd334e4e-d4db-4fb5-802e-8809f090f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.db.get_table_schema(table_name=\"product_subcategory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e93824-4f7b-42a0-94c7-92dfc5fa6b67",
   "metadata": {},
   "source": [
    "## Clean product data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77abd0ab-0b79-4e98-9bde-55cb09ca51cd",
   "metadata": {},
   "source": [
    "To clean the product data and separate it into three dataframes to normalize the data, you will use a pre-prepared function, `clean_product_data`. The cleaning code is displayed below, and then invoked to clean the raw product data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68245d77-d37e-49a4-9e04-c2df620684b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat inspect.getsource(tutorial.cookbook2.clean_product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa3c45b-a5ea-4aa0-9cab-74beafa063b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products, df_product_categories, df_product_subcategories = (\n",
    "    tutorial.cookbook2.clean_product_data(df_products_raw)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5e7e37-618f-481a-8ad6-704782c0b117",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {df_products.shape[0]} cleaned product rows.\\n\")\n",
    "\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25489fb3-368b-4873-a402-b759fe9de53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {df_product_categories.shape[0]} cleaned product category rows.\\n\")\n",
    "\n",
    "df_product_categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed5ebc-b58d-419c-8dfe-b25f94f861e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {df_product_subcategories.shape[0]} cleaned product subcategory rows.\\n\")\n",
    "\n",
    "df_product_subcategories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684b1b89-78f0-4b0a-8f06-398815ce56e2",
   "metadata": {},
   "source": [
    "## GX data validation workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee9db7-1495-450f-a784-f392120185ab",
   "metadata": {},
   "source": [
    "You will validate the cleaned product data using GX prior to loading it into a Postgres database table.\n",
    "\n",
    "The GX data validation workflow was introduced in [Cookbook 1](Cookbook_1_Validate_data_during_ingestion_happy_path.ipynb), which provided a walkthrough of the following GX components:\n",
    "* Data Context\n",
    "* Data Source\n",
    "* Data Asset\n",
    "* Batch Definition\n",
    "* Batch\n",
    "* Expectation\n",
    "* Expectation Suite\n",
    "* Validation Result\n",
    "\n",
    "This cookbook will extend the GX validation workflow to include the Validation Definition and Checkpoint components, and will further explore the validation metadata returned in the Validation Result.\n",
    "\n",
    "This tutorial contains concise explanations of GX components and workflows. For more detail, visit the [Introduction to GX Core](https://docs.greatexpectations.io/docs/core/introduction/) in the GX docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a2b58f-f1ca-4b9a-83e2-19a647a171b0",
   "metadata": {},
   "source": [
    "### Set up the GX validation workflow\n",
    "\n",
    "This validation will create the following Expectations:\n",
    "* Expect that the product dataset contains the following columns, in the specified order\n",
    "* Expect that all product unit prices are at least $1 USD\n",
    "* Expect that all products have a higher unit price than unit cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df334b32-703c-4c3f-932a-0940ca02b035",
   "metadata": {},
   "source": [
    "```{admonition} Reminder: Adding GX components to the Data Context\n",
    "GX components are unique on name. Once a component is created with the Data Context, adding another component with the same name will cause an error. To enable repeated execution of cookbook cells that add GX workflow components, you will see the following pattern:\n",
    "\n",
    "    try:\n",
    "        Add a new component(s) to the context\n",
    "    except:\n",
    "        Get component(s) from the context by name, or delete and recreate the component(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cae696e-209b-48be-8104-a2b30ffae51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Data Context.\n",
    "context = gx.get_context()\n",
    "\n",
    "# Create the Data Source, Data Asset, and Batch Definition.\n",
    "try:\n",
    "    data_source = context.data_sources.add_pandas(\"pandas\")\n",
    "    data_asset = data_source.add_dataframe_asset(name=\"customer data\")\n",
    "    batch_definition = data_asset.add_batch_definition_whole_dataframe(\n",
    "        \"batch definition\"\n",
    "    )\n",
    "\n",
    "except:\n",
    "    data_source = context.data_sources.get(\"pandas\")\n",
    "    data_asset = data_source.get_asset(name=\"customer data\")\n",
    "    batch_definition = data_asset.get_batch_definition(\"batch definition\")\n",
    "\n",
    "# Get the Batch from the Batch Definition.\n",
    "batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df_products})\n",
    "\n",
    "# Create the Expectation Suite.\n",
    "try:\n",
    "    expectation_suite = context.suites.add(\n",
    "        gx.ExpectationSuite(name=\"product expectations\")\n",
    "    )\n",
    "except:\n",
    "    expectation_suite = context.suites.delete(name=\"product expectations\")\n",
    "    expectation_suite = context.suites.add(\n",
    "        gx.ExpectationSuite(name=\"product expectations\")\n",
    "    )\n",
    "\n",
    "expectations = [\n",
    "    gxe.ExpectTableColumnsToMatchOrderedList(\n",
    "        column_list=[\n",
    "            \"product_id\",\n",
    "            \"name\",\n",
    "            \"brand\",\n",
    "            \"color\",\n",
    "            \"unit_cost_usd\",\n",
    "            \"unit_price_usd\",\n",
    "            \"product_category_id\",\n",
    "            \"product_subcategory_id\",\n",
    "        ]\n",
    "    ),\n",
    "    gxe.ExpectColumnValuesToBeBetween(column=\"unit_price_usd\", min_value=1.0),\n",
    "    gxe.ExpectColumnPairValuesAToBeGreaterThanB(\n",
    "        column_A=\"unit_price_usd\", column_B=\"unit_cost_usd\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for expectation in expectations:\n",
    "    expectation_suite.add_expectation(expectation)\n",
    "\n",
    "validation_result = batch.validate(expectation_suite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff73224-938a-471c-9dc9-ebdc6c4e54bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result[\"success\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4401526-dc15-4392-90d8-469e6a4dee62",
   "metadata": {},
   "source": [
    "### Extend the validation workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f51c51-61b5-4816-a64e-df57159d9e5a",
   "metadata": {},
   "source": [
    "A **Validation Definition** pairs a Batch Definition with an Expectation Suite. It defines what data you want to validate using which Expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908e39e0-da9d-44a9-aab8-1e60c35e7fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Validation Definition.\n",
    "try:\n",
    "    validation_definition = context.validation_definitions.add(\n",
    "        gx.ValidationDefinition(\n",
    "            name=\"product validation definition\",\n",
    "            data=batch_definition,\n",
    "            suite=expectation_suite,\n",
    "        )\n",
    "    )\n",
    "except:\n",
    "    context.validation_definitions.delete(name=\"product validation definition\")\n",
    "    validation_definition = context.validation_definitions.add(\n",
    "        gx.ValidationDefinition(\n",
    "            name=\"product validation definition\",\n",
    "            data=batch_definition,\n",
    "            suite=expectation_suite,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26890cd8-53b9-4904-ae97-bd895748588b",
   "metadata": {},
   "source": [
    "A **Checkpoint** executes data validation based on the specifications of the Validation Definition. Checkpoints also enable actions to be tied to data validation, and \n",
    "\n",
    "`unexpected_index_column_names`\n",
    "\n",
    "Result format: https://docs.greatexpectations.io/docs/core/trigger_actions_based_on_results/choose_a_result_format/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc1d65b-db64-44fe-bd10-4f087ed4594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Checkpoint.\n",
    "try:\n",
    "    checkpoint = context.checkpoints.add(\n",
    "        gx.Checkpoint(\n",
    "            name=\"checkpoint\",\n",
    "            validation_definitions=[validation_definition],\n",
    "            result_format={\n",
    "                \"result_format\": \"COMPLETE\",\n",
    "                # \"include_unexpected_rows\": True,\n",
    "                # \"exclude_unexpected_values\": True,\n",
    "                \"unexpected_index_column_names\": [\"product_id\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "except:\n",
    "    context.checkpoints.delete(name=\"checkpoint\")\n",
    "    checkpoint = context.checkpoints.add(\n",
    "        gx.Checkpoint(\n",
    "            name=\"checkpoint\",\n",
    "            validation_definitions=[validation_definition],\n",
    "            result_format={\n",
    "                \"result_format\": \"COMPLETE\",\n",
    "                # \"include_unexpected_rows\": True,\n",
    "                # \"exclude_unexpected_values\": True,\n",
    "                \"unexpected_index_column_names\": [\"product_id\"],\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd65b819-588b-419d-a82c-414f4f7e1dc3",
   "metadata": {},
   "source": [
    "Next, run the Checkpoint. When validating dataframe Data Sources, the dataframe must be supplied to the Checkpoint at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a798e3-dd87-4513-b757-134a94ad4edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_result = checkpoint.run(batch_parameters={\"dataframe\": df_products})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7faa28-ea0f-40d3-a954-90df8e6993a5",
   "metadata": {},
   "source": [
    "## Examine Validation Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e12ed7-be50-40a4-8d7c-d6d493baee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Validation Result object from the Checkpoint results.\n",
    "validation_result = checkpoint_result.run_results[\n",
    "    list(checkpoint_result.run_results.keys())[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd62090-1003-4560-9f60-7c198ba968d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result[\"success\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9597a-9c02-44a5-a29c-4f5f8f7aa1c0",
   "metadata": {},
   "source": [
    "```\n",
    " \"statistics\": {\n",
    "    \"evaluated_expectations\": 3,\n",
    "    \"successful_expectations\": 2,\n",
    "    \"unsuccessful_expectations\": 1,\n",
    "    \"success_percent\": 66.66666666666666\n",
    "  },\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc43a93-a931-482c-8e8b-1c2d077decc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectations_run = validation_result[\"statistics\"][\"evaluated_expectations\"]\n",
    "expectations_failed = validation_result[\"statistics\"][\"unsuccessful_expectations\"]\n",
    "\n",
    "print(\n",
    "    f\"{expectations_run} Expectations were run, {expectations_failed} Expectations failed.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05770580-3fbe-4c55-b8c9-0a9a2535a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_expectations = []\n",
    "for result in validation_result[\"results\"]:\n",
    "    if result[\"success\"] is True:\n",
    "        failed_expectations.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f1e408-d51b-45af-a69a-3080955a70d2",
   "metadata": {},
   "source": [
    "## pull out bad rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3190040d-a52e-42d7-8f6d-fe19470677e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_expectation = [\n",
    "    x\n",
    "    for x in validation_result[\"results\"]\n",
    "    if x[\"expectation_config\"][\"type\"] == \"expect_column_values_to_be_between\"\n",
    "][0]\n",
    "failed_expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc45a6bc-2e12-44ef-abaa-218362ecb6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_expectation[\"result\"][\"unexpected_index_list\"]\n",
    "bad_product_ids = [\n",
    "    x[\"product_id\"] for x in failed_expectation[\"result\"][\"unexpected_index_list\"]\n",
    "]\n",
    "bad_product_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b54b2d4-3d63-4569-86c2-c26362af0949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out bad rows from original product dataset.\n",
    "df_products[df_products[\"product_id\"].isin(bad_product_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880a999-7abf-4684-9a8d-9afbbfd9e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the bad rows.\n",
    "df_products_validated = df_products.drop(\n",
    "    df_products[df_products[\"product_id\"].isin(bad_product_ids)].index\n",
    ").reset_index(drop=True)\n",
    "\n",
    "df_products_validated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41880604-b5cc-4c34-87e9-389bfbc8da78",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    products_validation_result,\n",
    "    product_category_validation_result,\n",
    "    product_category_validation_result,\n",
    ") = tutorial.cookbook2.validate_product_data(\n",
    "    df_products, df_product_categories, df_product_subcategories\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd532b9d-e720-4676-9cf3-998548bdf1ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
