{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Cookbook 2: Validate data during ingestion (take action on failures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "This cookbook showcases a sample GX data validation workflow characteristic of data ingestion at the start of the data pipeline. Data is loaded into a Pandas DataFrame, cleaned, validated, invalid data is identified and removed, and then valid data is ingested into a Postgres database table.\n",
    "\n",
    "This cookbook explores the validation workflow first in a notebook setting, then embedded within an Airflow pipeline. Airflow pipelines are also referred to as directed acyclic graphs, or DAGs.\n",
    "\n",
    "This cookbook features a scenario in which a subset of data fails validation and must be handled in the pipeline.\n",
    "\n",
    "This cookbook builds on [Cookbook 1: Validate data during ingestion (happy path)](Cookbook_1_Validate_data_during_ingestion_happy_path.ipynb) and focuses on how data validation failures can be programmatically handled in the pipeline based on GX Validation Results. This cookbook assumes basic familiarity with GX Core workflows; for a step-by-step explanation of the GX data validation workflow, refer to [Cookbook 1](Cookbook_1_Validate_data_during_ingestion_happy_path.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "This tutorial features the `great_expectations` library.\n",
    "\n",
    "The `tutorial_code` module contains helper functions used within this notebook and the associated Airflow pipeline.\n",
    "\n",
    "The `airflow_dags` submodule is included so that you can inspect the code used in the related Airflow DAG directly from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "import great_expectations as gx\n",
    "import great_expectations.expectations as gxe\n",
    "import IPython\n",
    "import pandas as pd\n",
    "\n",
    "import tutorial_code as tutorial\n",
    "import airflow_dags.cookbook2_validate_and_handle_invalid_data as dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "In this tutorial, you will clean and validate a dataset containing synthesized product data. The data is loaded from a CSV file into a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = pathlib.Path(\"/cookbooks/data/raw\")\n",
    "\n",
    "df_products_raw = pd.read_csv(DATA_DIR / \"products.csv\", encoding=\"unicode_escape\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {df_products_raw.shape[0]} product rows into dataframe.\\n\")\n",
    "\n",
    "display(df_products_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Examine destination tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "The product data will be normalized and loaded into multiple Postgres tables:\n",
    "* `products`\n",
    "* `product_category`\n",
    "* `product_subcategory`\n",
    "\n",
    "Examine the schema of the destination tables and compare to the initial schema and contents of the raw product data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.db.get_table_schema(table_name=\"products\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.db.get_table_schema(table_name=\"product_category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.db.get_table_schema(table_name=\"product_subcategory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Clean product data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "To clean the product data and separate it into three DataFrames to normalize the data, you will use a pre-prepared function, `clean_product_data`. The cleaning code is displayed below, and then invoked to clean the raw product data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat inspect.getsource(tutorial.cookbook2.clean_product_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products, df_product_categories, df_product_subcategories = (\n",
    "    tutorial.cookbook2.clean_product_data(df_products_raw)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "Examine the cleaned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {df_products.shape[0]} cleaned product rows.\\n\")\n",
    "\n",
    "df_products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {df_product_categories.shape[0]} cleaned product category rows.\\n\")\n",
    "\n",
    "df_product_categories.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {df_product_subcategories.shape[0]} cleaned product subcategory rows.\\n\")\n",
    "\n",
    "df_product_subcategories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## GX data validation workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "You will validate the cleaned product data using GX prior to loading it into a Postgres database table.\n",
    "\n",
    "The GX data validation workflow was introduced in [Cookbook 1](Cookbook_1_Validate_data_during_ingestion_happy_path.ipynb), which provided a walkthrough of the following GX components:\n",
    "* Data Context\n",
    "* Data Source\n",
    "* Data Asset\n",
    "* Batch Definition\n",
    "* Batch\n",
    "* Expectation\n",
    "* Expectation Suite\n",
    "* Validation Result\n",
    "\n",
    "This cookbook will extend the GX validation workflow to include the Validation Definition and Checkpoint components, and will further explore the validation metadata returned in the Validation Result.\n",
    "\n",
    "This tutorial contains concise explanations of GX components and workflows. For more detail, visit the [Introduction to GX Core](https://docs.greatexpectations.io/docs/core/introduction/) in the GX docs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Set up the GX validation workflow\n",
    "\n",
    "Create a data validation workflow, up to Expectation Suite definition, that expects the following of your product data:\n",
    "* Expect that the product dataset contains the following columns, in the specified order:\n",
    "\n",
    "   `product_id`, `name`, `brand`, `color`, `unit_cost_usd`, `unit_price_usd`, `product_category_id`, `product_subcategory_id`\n",
    "* Expect that all product unit prices are at least $1 USD\n",
    "* Expect that all products have a higher unit price than unit cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "```{admonition} Reminder: Adding GX components to the Data Context\n",
    "GX components are unique on name. Once a component is created with the Data Context, adding another component with the same name will cause an error. To enable repeated execution of cookbook cells that add GX workflow components, you will see the following pattern:\n",
    "\n",
    "    try:\n",
    "        Add a new component(s) to the context\n",
    "    except:\n",
    "        Get component(s) from the context by name, or delete and recreate the component(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Data Context.\n",
    "context = gx.get_context()\n",
    "\n",
    "# Create the Data Source, Data Asset, and Batch Definition.\n",
    "try:\n",
    "    data_source = context.data_sources.add_pandas(\"pandas\")\n",
    "    data_asset = data_source.add_dataframe_asset(name=\"customer data\")\n",
    "    batch_definition = data_asset.add_batch_definition_whole_dataframe(\n",
    "        \"batch definition\"\n",
    "    )\n",
    "\n",
    "except:\n",
    "    data_source = context.data_sources.get(\"pandas\")\n",
    "    data_asset = data_source.get_asset(name=\"customer data\")\n",
    "    batch_definition = data_asset.get_batch_definition(\"batch definition\")\n",
    "\n",
    "# Get the Batch from the Batch Definition.\n",
    "batch = batch_definition.get_batch(batch_parameters={\"dataframe\": df_products})\n",
    "\n",
    "# Create the Expectation Suite.\n",
    "try:\n",
    "    expectation_suite = context.suites.add(\n",
    "        gx.ExpectationSuite(name=\"product expectations\")\n",
    "    )\n",
    "except:\n",
    "    expectation_suite = context.suites.delete(name=\"product expectations\")\n",
    "    expectation_suite = context.suites.add(\n",
    "        gx.ExpectationSuite(name=\"product expectations\")\n",
    "    )\n",
    "\n",
    "expectations = [\n",
    "    gxe.ExpectTableColumnsToMatchOrderedList(\n",
    "        column_list=[\n",
    "            \"product_id\",\n",
    "            \"name\",\n",
    "            \"brand\",\n",
    "            \"color\",\n",
    "            \"unit_cost_usd\",\n",
    "            \"unit_price_usd\",\n",
    "            \"product_category_id\",\n",
    "            \"product_subcategory_id\",\n",
    "        ]\n",
    "    ),\n",
    "    gxe.ExpectColumnValuesToBeBetween(column=\"unit_price_usd\", min_value=1.0),\n",
    "    gxe.ExpectColumnPairValuesAToBeGreaterThanB(\n",
    "        column_A=\"unit_price_usd\", column_B=\"unit_cost_usd\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for expectation in expectations:\n",
    "    expectation_suite.add_expectation(expectation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Extend the validation workflow to include Validation Definition and Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "A **Validation Definition** pairs a Batch Definition with an Expectation Suite. It defines what data you want to validate using which Expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Validation Definition.\n",
    "try:\n",
    "    validation_definition = context.validation_definitions.add(\n",
    "        gx.ValidationDefinition(\n",
    "            name=\"product validation definition\",\n",
    "            data=batch_definition,\n",
    "            suite=expectation_suite,\n",
    "        )\n",
    "    )\n",
    "except:\n",
    "    context.validation_definitions.delete(name=\"product validation definition\")\n",
    "    validation_definition = context.validation_definitions.add(\n",
    "        gx.ValidationDefinition(\n",
    "            name=\"product validation definition\",\n",
    "            data=batch_definition,\n",
    "            suite=expectation_suite,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "A **Checkpoint** executes data validation based on the specifications of the Validation Definition. \n",
    "\n",
    "Checkpoints return Checkpoint Result objects, which contain Validation Result objects for individual runs. Checkpoints also enable you to specify the level of detail that is returned in your Validation Results. This is done using the `result_format` parameter. \n",
    "\n",
    "For a comprehensive description on levels of detail offered by result format settings, visit [Choose result format](https://docs.greatexpectations.io/docs/core/trigger_actions_based_on_results/choose_a_result_format/) in the GX docs.\n",
    "\n",
    "In the code below:\n",
    "* The result format is set to `COMPLETE`, which returns all possible validation metadata in the Validation Result.\n",
    "* `unexpected_index_column_names` is set to the `product_id` column, meaning that any unexpected (failing) rows will be identified by the `product_id` column in the Validation Result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Checkpoint.\n",
    "try:\n",
    "    checkpoint = context.checkpoints.add(\n",
    "        gx.Checkpoint(\n",
    "            name=\"checkpoint\",\n",
    "            validation_definitions=[validation_definition],\n",
    "            result_format={\n",
    "                \"result_format\": \"COMPLETE\",\n",
    "                \"unexpected_index_column_names\": [\"product_id\"],\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "except:\n",
    "    context.checkpoints.delete(name=\"checkpoint\")\n",
    "    checkpoint = context.checkpoints.add(\n",
    "        gx.Checkpoint(\n",
    "            name=\"checkpoint\",\n",
    "            validation_definitions=[validation_definition],\n",
    "            result_format={\n",
    "                \"result_format\": \"COMPLETE\",\n",
    "                \"unexpected_index_column_names\": [\"product_id\"],\n",
    "            },\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Next, run the Checkpoint. When validating DataFrame Data Assets, the DataFrame must be supplied to the Checkpoint at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_result = checkpoint.run(batch_parameters={\"dataframe\": df_products})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "The Checkpoint run returns a `CheckpointResult` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(checkpoint_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Examine Validation Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "The Validation Result object can be extracted from the Checkpoint Result object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the Validation Result object from the Checkpoint results.\n",
    "validation_result = checkpoint_result.run_results[\n",
    "    list(checkpoint_result.run_results.keys())[0]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### Get summary information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "The Validation Result `success` field indicates whether or not all Expectations passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result[\"success\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Another useful Validation Results summary field is `statistics`, which provides an overview of how many Expectations passed and how many failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result[\"statistics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectations_run = validation_result[\"statistics\"][\"evaluated_expectations\"]\n",
    "expectations_failed = validation_result[\"statistics\"][\"unsuccessful_expectations\"]\n",
    "\n",
    "print(\n",
    "    f\"{expectations_run} Expectations were run, {expectations_failed} Expectations failed.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Retrieve results for individual Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "The `results` field contains individual results for each Expectation. You can use the information contained in `results` (the level of detail of results is specified by the Checkpoint `result_format` parameter) to identify why Expectations failed, and what rows failed validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_expectations = []\n",
    "for result in validation_result[\"results\"]:\n",
    "    if result[\"success\"] is False:\n",
    "        failed_expectations.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "If you examine the results of one of the failed Expectation, you can see that the Validation Result provides a `unexpected_index_list` field containing a list of the values that failed validation. Each element of the list represents a row in the original dataset, and the fields present are those directly used in the Expectation (for example, `unit_price_usd` and `unit_cost_usd` for the failed `ExpectColumnPairValuesAToBeGreaterThanB` Expectation), and any other fields specified using the `result_format` `unexpected_index_column_names` field.\n",
    "\n",
    "Based on the definition of the Checkpoint above, the records in the `unexpected_index_list` will also contain the `product_id` field, so that they can be used to identify the original rows in the Data Asset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_expectations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_expectations[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Use the Validation Result to separate valid and invalid rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "You can use the metadata provided in the Validation Results `unexpected_index_list` to identify the original rows in the Data Asset, enabling separation of valid and invalid rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "As an example, extract the `product_ids` of the product rows that failed the first Expectation and use those ids to create two separate dataframes, one containing valid product rows and one containing the rows that failed the Expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the metadata contained in the Validation Results unexpected_index_list field\n",
    "# for the failing Expectation.\n",
    "failed_expectations[0][\"result\"][\"unexpected_index_list\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the product ids.\n",
    "product_ids_for_invalid_rows = [\n",
    "    x[\"product_id\"] for x in failed_expectations[0][\"result\"][\"unexpected_index_list\"]\n",
    "]\n",
    "\n",
    "product_ids_for_invalid_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out bad rows from original product dataset.\n",
    "df_invalid_rows = df_products[\n",
    "    df_products[\"product_id\"].isin(product_ids_for_invalid_rows)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "display(df_invalid_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the invalid rows from the product data.\n",
    "df_products_validated = df_products.drop(\n",
    "    df_products[df_products[\"product_id\"].isin(product_ids_for_invalid_rows)].index\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# Verify that product data contains the correct number of rows.\n",
    "assert df_products_validated.shape[0] == (\n",
    "    df_products.shape[0] - len(product_ids_for_invalid_rows)\n",
    ")\n",
    "\n",
    "display(df_products_validated.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## Integrate GX validation and programmatic handling of invalid rows in the Airflow DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "You will use the `success` and `unexpected_index_list` metadata of the GX Validation Result object to control the actions of the `cookbook2_validate_and_handle_invalid_data` Airflow pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "### Inspect DAG code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "Examine the DAG code below that defines the `cookbook2_validate_and_handle_invalid_data` pipeline.\n",
    "\n",
    "The DAG code cleans the incoming product data and then validates the data. Rows that pass validation are written to Postgres, but rows that fail validation trigger an action in the pipeline:\n",
    "* Failing product category and subcategory rows cause the pipeline to raise an error and halt.\n",
    "\n",
    "  ```\n",
    "  # Halt pipeline with error if validation fails for product category or subcategory results.\n",
    "  if not product_category_validation_result[\"success\"]:\n",
    "      raise Exception(\"GX data validation for product categories failed.\")\n",
    "\n",
    "  if not product_subcategory_validation_result[\"success\"]:\n",
    "      raise Exception(\"GX data validation for product subcategories failed.\")\n",
    "  ```\n",
    "\n",
    "* Failing product rows are automatically separated from the valid rows and written to an error file (`cookbook2_invalid_product_rows.csv`).\n",
    "\n",
    "  ```\n",
    "  # If validation fails for product rows, automatically remove failing rows and write\n",
    "  # to error file. Write all remaining valid rows to Postgres.\n",
    "  if not products_validation_result[\"success\"]:\n",
    "      df_products_valid, df_products_invalid = (\n",
    "          tutorial.cookbook2.separate_valid_and_invalid_product_rows(\n",
    "              df_products, products_validation_result\n",
    "          )\n",
    "      )\n",
    "      tutorial.cookbook2.write_invalid_rows_to_file(\n",
    "          OUTPUT_DATA_DIR / \"cookbook2_invalid_product_rows.csv\", df_products_invalid\n",
    "      )\n",
    "\n",
    "  else:\n",
    "      df_products_valid = df_products\n",
    "\n",
    "  product_rows_inserted = tutorial.db.insert_ignore_dataframe_to_postgres(\n",
    "      table_name=\"products\", dataframe=df_products_valid\n",
    "  )\n",
    "  ```\n",
    "  *  The error file is made available in the `airflow_pipeline_output` directory, located in the same directory as this cookbook.\n",
    "  *  The code in `tutorial.cookbook2.separate_valid_and_invalid_product_rows` utilizes the same technique shown above to identify invalid rows from the Validation Result, extract the relevant row id, and separate the original Pandas DataFrame into one DataFrame containing valid rows and one containing invalid rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat inspect.getsource(dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "## View the Airflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "To view the `cookbook2_validate_and_handle_invalid_data` pipeline in the Airflow UI, log into the locally running Airflow instance.\n",
    "\n",
    "1. Open [http://localhost:8080/](http://localhost:8080/) in a browser window.\n",
    "2. Log in with these credentials:\n",
    "  * Username: `admin`\n",
    "  * Password: `gx`\n",
    "\n",
    "You will see the pipeline under **DAGs** on login."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Video(\"img/screencaptures/log_in_to_airflow.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "You can trigger the DAG from this notebook, using the provided convenience function in the cell below, or you can trigger the DAG manually in the Airflow UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tutorial.airflow.trigger_airflow_dag_and_wait_for_run(\n",
    "    \"cookbook2_validate_and_handle_invalid_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "To trigger the `cookbook2_validate_and_handle_invalid_data` DAG from the Airflow UI, click the **Trigger DAG** button (with a play icon) under Actions. This will queue the DAG and it will execute shortly. The successful run is indicated by the run count inside the green circle under **Runs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Video(\"img/screencaptures/trigger_airflow_dag.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "The `cookbook2_validate_and_handle_invalid_data` DAG can be rerun multiple times; you can experiment with running it from this notebook or from the Airflow UI. The pipeline insert ignores into the Postgres `products`, `product_category`, and `product_subcategory` tables, meaning that it will not attempt to insert a row with the same primary key as an existing row."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "## View pipeline results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "Once the pipeline has been run, the `products` table is populated with the cleaned product data, a total of 15,266 rows. You can view the updated table counts and a sampling of rows below.\n",
    "\n",
    "* `products` is populated with 2510 rows\n",
    "* `product_category` is populated with 8 rows \n",
    "* `product_subcategory` is populated with 32 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Product rows: {tutorial.db.get_table_row_count(table_name='products')}\\n\")\n",
    "print(\"Sample product data from Postgres:\")\n",
    "\n",
    "pd.read_sql_query(\n",
    "    \"select * from products limit 5\", con=tutorial.db.get_local_postgres_engine()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Product category rows: {tutorial.db.get_table_row_count(table_name='product_category')}\\n\"\n",
    ")\n",
    "print(\"Sample product category data from Postgres:\")\n",
    "\n",
    "pd.read_sql_query(\n",
    "    \"select * from product_category limit 5\",\n",
    "    con=tutorial.db.get_local_postgres_engine(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Product subcategory rows: {tutorial.db.get_table_row_count(table_name='product_subcategory')}\\n\"\n",
    ")\n",
    "print(\"Sample product subcategory data from Postgres:\")\n",
    "\n",
    "pd.read_sql_query(\n",
    "    \"select * from product_subcategory limit 5\",\n",
    "    con=tutorial.db.get_local_postgres_engine(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Additionally, you can see that the invalid rows were written to the error file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat airflow_pipeline_output/cookbook2_invalid_product_rows.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "It can also be helpful to view the pipeline logs to investigate the details of a successful (or unsuccessful run). To examine these logs in the Airflow UI:\n",
    "1. On the DAGs screen, click on the run(s) of interest under Runs.\n",
    "2. Click the name of the individual run you want to examine. This will load the DAG execution details.\n",
    "3. Click the Graph tab, and then the `cookbook2_validate_and_handle_invalid_data` task box on the visual rendering.\n",
    "4. Click the Logs tab to load the DAG logs.\n",
    "\n",
    "You can see in the screen capture below that the logs reflect the row insertion print statement that was included in the DAG code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Video(\"img/screencaptures/cookbook2_run_dag.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "This cookbook has walked you through the process of validating data using GX, integrating the data validation workflow in an Airflow pipeline, and then programmatically handling invalid data in the pipeline when validation fails.\n",
    "\n",
    "[Cookbook 1](Cookbook_1_Validate_data_during_ingestion_happy_path.ipynb) and Cookbook 2 (this notebook) have focused on usage of [GX Core](https://docs.greatexpectations.io/docs/core/introduction/) to implement data validation in a data pipeline. Subsequent cookbooks will explore integrating [GX Cloud](https://docs.greatexpectations.io/docs/cloud/overview/gx_cloud_overview), GX Core, and an Airflow data pipeline to achieve end-to-end data validation workflows that make validation results available and shareable in the GX Cloud web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
